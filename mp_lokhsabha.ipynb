{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required library, Used Selenium and Beautifulsoup to extract data from web pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome driver to run selenium\n",
    "driver = webdriver.Chrome(executable_path='D:\\\\ISB\\\\chromedriver_win32\\\\\\\\chromedriver.exe')\n",
    "\n",
    "\n",
    "# seed url to get the data\n",
    "URL = \"http://loksabhaph.nic.in/Members/lokprev.aspx\"\n",
    "driver.get(URL)\n",
    "\n",
    "\n",
    "# To wait for the elements to be visible / rendered by the browser\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a tabular data and all the details are in alphabetical order. so we are trying to get hyperlinks for all the alphabets\n",
    "# separately and appending them to a list.\n",
    "\n",
    "\n",
    "soup1 = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "\n",
    "all_alpha = soup1.find('table', {'class': 'alphabets'})\n",
    "\n",
    "ea_alpha = all_alpha.find('tbody')\n",
    "\n",
    "all_aaa = ea_alpha.find_all('td')\n",
    "print(len(all_aaa))\n",
    "list_url = []\n",
    "for ea in ea_alpha.find_all('a', href=True): # for loop to append all the urls (for each alphabet)\n",
    "    # print(ea['href'])\n",
    "    list_url.append(ea['href'])\n",
    "    \n",
    "    \n",
    "static_url = 'http://loksabhaph.nic.in/Members/'\n",
    "\n",
    "res = []\n",
    "for each_url in list_ur:\n",
    "    alpha_url = static_url + each_url\n",
    "    driver.get(alpha_url)\n",
    "    \n",
    "    # we have a dropdown to select the Results per page. [0, 25, 50, 100, ALL] from dropdown we are selecting ALL to get totol data in single table for all alphabets\n",
    "    type = 'All'\n",
    "    select = Select(driver.find_element_by_id('ContentPlaceHolder1_drdpages'))\n",
    "    select.select_by_visible_text(type) # code for selecting the dropdown\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    time.sleep(1)\n",
    "    mem_list = soup.find('table', {'class': 'member_list_table'}) # looping through each and every alphabet url and trying to find the table of records\n",
    "\n",
    "    all_mp = mem_list.find('tbody') # from table extracting body part\n",
    "\n",
    "    each_row = all_mp.find_all('tr') # from body we are collecting all the rows to scrape\n",
    "    # print(len(each_row))\n",
    "    lista = []\n",
    "\n",
    "    for each in each_row: # looping over each row to get different values like Name, Party, State etc..\n",
    "        all_td = each.find_all('td') # find all the values from td tag indexing over them to extract the text.\n",
    "        results = dict()\n",
    "        # print(all_td[1])\n",
    "        get_hre = all_td[1].find('a', href=True) # we have separate page for each profile with extra info, so we are extracting href\n",
    "        print(get_hre['href'])\n",
    "        prof_url = stattt + get_hre['href']\n",
    "\n",
    "        driver.get(prof_url)\n",
    "        soupd = BeautifulSoup(driver.page_source,\"html.parser\") # parsing the url we got from the above code to fetch more info\n",
    "        # time.sleep(2)\n",
    "\n",
    "\n",
    "        # used try and except method for all the values, if some value are null or if we have any attributeError or Exception we can handle them using try n except method\n",
    "\n",
    "        try:\n",
    "            tab_d = soupd.find('table', {'id':'ContentPlaceHolder1_Datagrid1'}) # for fetching email data\n",
    "\n",
    "            pro_alpha = tab_d.find('tbody')\n",
    "\n",
    "            pro_aaa = pro_alpha.find_all('td',{'class':'griditem2'})\n",
    "            email = pro_aaa[2].text.strip()\n",
    "        except AttributeError:\n",
    "            email = \"NA\"\n",
    "        # print(\"email = \", pro_aaa)\n",
    "\n",
    "\n",
    "        try:\n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) # for fetching father name\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            father_name = pro_fa[0].text.strip()\n",
    "        except AttributeError:\n",
    "            father_name = \"NA\"\n",
    "        print(\"father_name = \", father_name)\n",
    "\n",
    "        try:\n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) # for fetching mother name\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            mother_name = pro_fa[1].text.strip()\n",
    "        except AttributeError:\n",
    "            mother_name = \"NA\"\n",
    "        print(\"mother_name = \", mother_name)\n",
    "\n",
    "\n",
    "        try:\n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) # date of birth\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            dob = pro_fa[2].text.strip()\n",
    "        except AttributeError:\n",
    "            dob = \"NA\"\n",
    "        print(\"dob = \", dob)\n",
    "\n",
    "\n",
    "        try:\n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) # place of birth\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            place_birth = pro_fa[3].text.strip()\n",
    "        except AttributeError:\n",
    "            place_birth = \"NA\"\n",
    "        print(\"place_birth = \", place_birth)\n",
    "\n",
    "\n",
    "        try:\n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) #Education\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            edu = pro_fa[9].text.strip()\n",
    "        except AttributeError:\n",
    "            edu = \"NA\"\n",
    "        print(\"Education = \", edu)\n",
    "\n",
    "        try:\n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) #Profession\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            profe = pro_fa[10].text.strip()\n",
    "        except AttributeError:\n",
    "            profe = \"NA\"\n",
    "        print(\"Profession = \", profe)\n",
    "\n",
    "        try:\n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) #Permanent Address\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            perm_add = pro_fa[11].text.strip()\n",
    "        except AttributeError:\n",
    "            perm_add = \"NA\"\n",
    "        print(\"Permanent_Address = \", perm_add)\n",
    "\n",
    "        try: \n",
    "            tab_d2 = soupd.find('table', {'id':'ContentPlaceHolder1_DataGrid2'}) #Present Address\n",
    "\n",
    "            pro_d = tab_d2.find('tbody')\n",
    "\n",
    "            pro_fa = pro_d.find_all('td',{'class':'griditem2'})\n",
    "            pres_add = pro_fa[12].text.strip()\n",
    "        except AttributeError:\n",
    "            pres_add = \"NA\"\n",
    "        print(\"Present_Address = \", pres_add)\n",
    "\n",
    "    \n",
    "\n",
    "        # created a dictionary called results and appended the values from the dict to list(res)\n",
    "        \n",
    "        results['Sno'] = all_td[0].text.strip().replace(',', '') #strip removes spaces at the beginning and at the end of the string \n",
    "        results['Name'] = all_td[1].text.strip().replace(',', '')# replace with remove the commas from the name and replace them with empty space\n",
    "        results['Profile_url'] = stattt + get_hre['href'] # extracted url to get more info about the candidate \n",
    "        results['Party'] = all_td[2].text.strip().replace(',', '')\n",
    "        results['State'] = all_td[3].text.strip().replace(',', '')\n",
    "        results['Exp'] = all_td[4].text.strip()\n",
    "        results['email'] = email\n",
    "        results['Father_Name'] = father_name\n",
    "        results['Mother_Name'] = mother_name\n",
    "        results['DOB'] = dob\n",
    "        results['Place_Of_Birth'] = place_birth\n",
    "        results['Education'] = edu\n",
    "        results['Profession'] = profe\n",
    "        results['Permanent_Address'] = perm_add\n",
    "        results['Present_Address'] = pres_add\n",
    "        res.append(results)\n",
    "        # lista.append({ 'SNo': all_td[0].text.strip().replace(',', ''), \"Name\": all_td[1].text.strip().replace(',', ''), \"Party\": all_td[2].text.strip().replace(',', ''), \"State\": all_td[3].text, \"Exp\": all_td[4].text, })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mp_loksabha.json', 'w') as f:  # dumping values to json\n",
    "    json.dump(res, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
